---
title: "Space-Time Prediction of Bike Share Demand: Philadelphia Indego"
author: "Zicheng Xiang"
date: "2025.11.23"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
```

# Introduction

## The Rebalancing Challenge in Philadelphia

Philadelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. 

Imagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have:
- 200 stations across Philadelphia
- Limited trucks and staff for moving bikes
- 2-3 hours before morning rush hour demand peaks
- **The question:** Which stations will run out of bikes by 8:30 AM?

This lab will teach you to build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.

## Learning Objectives

By the end of this assignment, you will be able to:

1. **Understand panel data structure** for space-time analysis
2. **Create temporal lag variables** to capture demand persistence
3. **Build multiple predictive models** with increasing complexity
4. **Validate models temporally** (train on past, test on future)
5. **Analyze prediction errors** in both space and time
6. **Engineer new features** based on error patterns
7. **Critically evaluate** when prediction errors matter most

## Assignment Structure

**Part 1 (In-Class/Together):** Work through Q1 2025 data following this code

**Part 2 (HW5 - Teams of 2):**
- Download a different quarter of Indego data (Q2, Q3, or Q4 2024)
- Apply the same workflow to your quarter
- Analyze error patterns in your quarter
- Add 2-3 new features to improve the model
- Write a brief report on what you learned

---

# Setup

## Load Libraries

```{r load_libraries}
# Core tidyverse
library(tidyverse)
library(lubridate)

# Spatial data
library(sf)
library(tigris)

# Census data
library(tidycensus)

# Weather data
library(riem)  # For Philadelphia weather from ASOS stations

# Visualization
library(viridis)
library(gridExtra)
library(knitr)
library(kableExtra)

# here!
library(here)
# Get rid of scientific notation. We gotta look good!
options(scipen = 999)
```

## Define Themes

```{r themes}
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

## Set Census API Key

```{r census_key, eval=FALSE}

census_api_key("e79f3706b6d61249968c6ce88794f6f556e5bf3d", overwrite = TRUE, install = TRUE)

```

```{r census_key_hidden, include=FALSE}
# Hidden key for rendering
census_api_key("e79f3706b6d61249968c6ce88794f6f556e5bf3d", overwrite = TRUE)
```

---

# Data Import & Preparation

## Load Indego Trip Data (Q1 2025)

**Note for HW5:** You'll download a different quarter from: https://www.rideindego.com/about/data/

```{r load_indego}
# Read Q1 2025 data
indego <- read_csv("data/indego-trips-2025-q1.csv")

# Quick look at the data
glimpse(indego)
```

## Examine the Data Structure

```{r explore_data}
# How many trips?
cat("Total trips in Q1 2025:", nrow(indego), "\n")

# Date range
cat("Date range:", 
    min(mdy_hm(indego$start_time)), "to", 
    max(mdy_hm(indego$start_time)), "\n")

# How many unique stations?
cat("Unique start stations:", length(unique(indego$start_station)), "\n")

# Trip types
table(indego$trip_route_category)

# Passholder types
table(indego$passholder_type)

# Bike types
table(indego$bike_type)
```

## Create Time Bins

We need to aggregate trips into hourly intervals for our panel data structure.

```{r create_time_bins}
indego <- indego %>%
  mutate(
    # Parse datetime
    start_datetime = mdy_hm(start_time),
    end_datetime = mdy_hm(end_time),
    
    # Create hourly bins
    interval60 = floor_date(start_datetime, unit = "hour"),
    
    # Extract time features
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    
    # Create useful indicators
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )

# Look at temporal features
head(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))
```

---

# Exploratory Analysis

## Trips Over Time

```{r trips_over_time}
# Daily trip counts
daily_trips <- indego %>%
  group_by(date) %>%
  summarize(trips = n())

ggplot(daily_trips, aes(x = date, y = trips)) +
  geom_line(color = "#3182bd", linewidth = 1) +
  geom_smooth(se = FALSE, color = "red", linetype = "dashed") +
  labs(
    title = "Indego Daily Ridership - Q1 2025",
    subtitle = "Winter demand patterns in Philadelphia",
    x = "Date",
    y = "Daily Trips",
    caption = "Source: Indego bike share"
  ) +
  plotTheme
```

**Question:** What patterns do you see? How does ridership change over time?

Notice a big spike in Februrary of 2025 (the 14th - not just Valentine's Day)? Was anybody in Philly last Feb? What closed everything and cancelled my PPA class? ... Hint: Fly Eagles Fly

Let's investigate...compare the 14th to all other Friday's in the data...what did you find? Keep this in mind for future Feature Engineering.

```{r}
fly_eagles_fly <- daily_trips %>% filter(date == "2025-02-14")

typical_boring_friday <- indego %>%
  filter(dotw == "Fri", date != "2025-02-14") %>%
  group_by(date) %>%
  summarize(trips = n()) %>%
  summarize(avg_friday_trips = mean(trips))

print(fly_eagles_fly)
print(typical_boring_friday)

```


## Hourly Patterns

```{r hourly_patterns}
# Average trips by hour and day type
hourly_patterns <- indego %>%
  group_by(hour, weekend) %>%
  summarize(avg_trips = n() / n_distinct(date)) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Average Hourly Ridership Patterns",
    subtitle = "Clear commute patterns on weekdays",
    x = "Hour of Day",
    y = "Average Trips per Hour",
    color = "Day Type"
  ) +
  plotTheme
```

**Question:** When are the peak hours? How do weekends differ from weekdays?

## Top Stations

```{r top_stations}
# Most popular origin stations
top_stations <- indego %>%
  count(start_station, start_lat, start_lon, name = "trips") %>%
  arrange(desc(trips)) %>%
  head(20)

kable(top_stations, 
      caption = "Top 20 Indego Stations by Trip Origins",
      format.args = list(big.mark = ",")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

---

# Get Philadelphia Spatial Context

## Load Philadelphia Census Data

We'll get census tract data to add demographic context to our stations.

```{r load_census}
# Get Philadelphia census tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",  # Total population
    "B19013_001",  # Median household income
    "B08301_001",  # Total commuters
    "B08301_010",  # Commute by transit
    "B02001_002",  # White alone
    "B25077_001"   # Median home value
  ),
  state = "PA",
  county = "Philadelphia",
  year = 2022,
  geometry = TRUE,
  output = "wide"
) %>%
  rename(
    Total_Pop = B01003_001E,
    Med_Inc = B19013_001E,
    Total_Commuters = B08301_001E,
    Transit_Commuters = B08301_010E,
    White_Pop = B02001_002E,
    Med_Home_Value = B25077_001E
  ) %>%
  mutate(
    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,
    Percent_White = (White_Pop / Total_Pop) * 100
  ) %>%
  st_transform(crs = 4326)  # WGS84 for lat/lon matching

# Check the data
glimpse(philly_census)
```

## Map Philadelphia Context

```{r map_philly}
# Map median income
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Context for understanding bike share demand patterns"
  ) +
  # Stations 
  geom_point(
    data = indego,
    aes(x = start_lon, y = start_lat),
    color = "red", size = 0.25, alpha = 0.6
  ) +
  mapTheme
```

## Join Census Data to Stations

We'll spatially join census characteristics to each bike station.

```{r join_census_to_stations}
# Create sf object for stations
stations_sf <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  st_as_sf(coords = c("start_lon", "start_lat"), crs = 4326)

# Spatial join to get census tract for each station
stations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%
  st_drop_geometry()

# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.

stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Add back to trip data
indego_census <- indego %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )


# Prepare data for visualization
stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Create the map showing problem stations
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = "white", size = 0.1) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar,
    na.value = "grey90"
  ) +
  # Stations with census data (small grey dots)
  geom_point(
    data = stations_for_map %>% filter(has_census),
    aes(x = start_lon, y = start_lat),
    color = "grey30", size = 1, alpha = 0.6
  ) +
  # Stations WITHOUT census data (red X marks the spot)
  geom_point(
    data = stations_for_map %>% filter(!has_census),
    aes(x = start_lon, y = start_lat),
    color = "red", size = 1, shape = 4, stroke = 1.5
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Indego stations shown (RED = no census data match)",
    caption = "Red X marks indicate stations that didn't join to census tracts"
  ) +
  mapTheme



```

# Dealing with missing data

We need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..


```{r}
# Identify which stations to keep
valid_stations <- stations_census %>%
  filter(!is.na(Med_Inc)) %>%
  pull(start_station)

# Filter trip data to valid stations only
indego_census <- indego %>%
  filter(start_station %in% valid_stations) %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )

```


# Get Weather Data

Weather significantly affects bike share demand! Let's get hourly weather for Philadelphia.

```{r get_weather}
# Get weather from Philadelphia International Airport (KPHL)
# This covers Q1 2025: January 1 - March 31
weather_data <- riem_measures(
  station = "PHL",  # Philadelphia International Airport
  date_start = "2025-01-01",
  date_end = "2025-03-31"
)

# Process weather data
weather_processed <- weather_data %>%
  mutate(
    interval60 = floor_date(valid, unit = "hour"),
    Temperature = tmpf,  # Temperature in Fahrenheit
    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches
    Wind_Speed = sknt  # Wind speed in knots
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct()

# Check for missing hours and interpolate if needed
weather_complete <- weather_processed %>%
  complete(interval60 = seq(min(interval60), max(interval60), by = "hour")) %>%
  fill(Temperature, Precipitation, Wind_Speed, .direction = "down")

# Look at the weather
summary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))
```

## Visualize Weather Patterns

Who is ready for a Philly winter?!

```{r visualize_weather}
ggplot(weather_complete, aes(x = interval60, y = Temperature)) +
  geom_line(color = "#3182bd", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "red") +
  labs(
    title = "Philadelphia Temperature - Q1 2025",
    subtitle = "Winter to early spring transition",
    x = "Date",
    y = "Temperature (°F)"
  ) +
  plotTheme
```

---

# Create Space-Time Panel

## Aggregate Trips to Station-Hour Level

```{r aggregate_trips}
# Count trips by station-hour
trips_panel <- indego_census %>%
  group_by(interval60, start_station, start_lat, start_lon,
           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%
  summarize(Trip_Count = n()) %>%
  ungroup()

# How many station-hour observations?
nrow(trips_panel)

# How many unique stations?
length(unique(trips_panel$start_station))

# How many unique hours?
length(unique(trips_panel$interval60))
```

## Create Complete Panel Structure

Not every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).

```{r complete_panel}
# Calculate expected panel size
n_stations <- length(unique(trips_panel$start_station))
n_hours <- length(unique(trips_panel$interval60))
expected_rows <- n_stations * n_hours

cat("Expected panel rows:", format(expected_rows, big.mark = ","), "\n")
cat("Current rows:", format(nrow(trips_panel), big.mark = ","), "\n")
cat("Missing rows:", format(expected_rows - nrow(trips_panel), big.mark = ","), "\n")

# Create complete panel
study_panel <- expand.grid(
  interval60 = unique(trips_panel$interval60),
  start_station = unique(trips_panel$start_station)
) %>%
  # Join trip counts
  left_join(trips_panel, by = c("interval60", "start_station")) %>%
  # Replace NA trip counts with 0
  mutate(Trip_Count = replace_na(Trip_Count, 0))

# Fill in station attributes (they're the same for all hours)
station_attributes <- trips_panel %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    Med_Inc = first(Med_Inc),
    Percent_Taking_Transit = first(Percent_Taking_Transit),
    Percent_White = first(Percent_White),
    Total_Pop = first(Total_Pop)
  )

study_panel <- study_panel %>%
  left_join(station_attributes, by = "start_station")

# Verify we have complete panel
cat("Complete panel rows:", format(nrow(study_panel), big.mark = ","), "\n")
```

## Add Time Features

```{r add_time_features}
study_panel <- study_panel %>%
  mutate(
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

## Join Weather Data

```{r join_weather}
study_panel <- study_panel %>%
  left_join(weather_complete, by = "interval60")

# Check for missing values
summary(study_panel %>% select(Trip_Count, Temperature, Precipitation))
```

---

# Create Temporal Lag Variables

The key innovation for space-time prediction: **past demand predicts future demand**.

## Why Lags?

If there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.

```{r create_lags}
# Sort by station and time
study_panel <- study_panel %>%
  arrange(start_station, interval60)

# Create lag variables WITHIN each station
study_panel <- study_panel %>%
  group_by(start_station) %>%
  mutate(
    lag1Hour = lag(Trip_Count, 1),
    lag2Hours = lag(Trip_Count, 2),
    lag3Hours = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day = lag(Trip_Count, 24)
  ) %>%
  ungroup()

# Remove rows with NA lags (first 24 hours for each station)
study_panel_complete <- study_panel %>%
  filter(!is.na(lag1day))

cat("Rows after removing NA lags:", format(nrow(study_panel_complete), big.mark = ","), "\n")
```

## Visualize Lag Correlations

```{r lag_correlations}
# Sample one station to visualize
example_station <- study_panel_complete %>%
  filter(start_station == first(start_station)) %>%
  head(168)  # One week

# Plot actual vs lagged demand
ggplot(example_station, aes(x = interval60)) +
  geom_line(aes(y = Trip_Count, color = "Current"), linewidth = 1) +
  geom_line(aes(y = lag1Hour, color = "1 Hour Ago"), linewidth = 1, alpha = 0.7) +
  geom_line(aes(y = lag1day, color = "24 Hours Ago"), linewidth = 1, alpha = 0.7) +
  scale_color_manual(values = c(
    "Current" = "#08519c",
    "1 Hour Ago" = "#3182bd",
    "24 Hours Ago" = "#6baed6"
  )) +
  labs(
    title = "Temporal Lag Patterns at One Station",
    subtitle = "Past demand predicts future demand",
    x = "Date-Time",
    y = "Trip Count",
    color = "Time Period"
  ) +
  plotTheme
```

---

# Temporal Train/Test Split

**CRITICAL:** We must train on PAST data and test on FUTURE data!

## Why Temporal Validation Matters

In real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).

**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)

**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)

```{r temporal_split}
# Split by week
# Q1 has weeks 1-13 (Jan-Mar)
# Train on weeks 1-9 (Jan 1 - early March)
# Test on weeks 10-13 (rest of March)

# Which stations have trips in BOTH early and late periods?
early_stations <- study_panel_complete %>%
  filter(week < 10) %>%
  filter(Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

late_stations <- study_panel_complete %>%
  filter(week >= 10) %>%
  filter(Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

# Keep only stations that appear in BOTH periods
common_stations <- intersect(early_stations, late_stations)


# Filter panel to only common stations
study_panel_complete <- study_panel_complete %>%
  filter(start_station %in% common_stations)

# NOW create train/test split
train <- study_panel_complete %>%
  filter(week < 10)

test <- study_panel_complete %>%
  filter(week >= 10)

cat("Training observations:", format(nrow(train), big.mark = ","), "\n")
cat("Testing observations:", format(nrow(test), big.mark = ","), "\n")
cat("Training date range:", min(train$date), "to", max(train$date), "\n")
cat("Testing date range:", min(test$date), "to", max(test$date), "\n")






```

---

# Build Predictive Models

We'll build 5 models with increasing complexity to see what improves predictions.

## Model 1: Baseline (Time + Weather)

```{r model1}
# Create day of week factor (R automatically uses treatment coding)
train <- train %>%
  mutate(dotw_simple = as.factor(dotw))

test <- test %>%
  mutate(dotw_simple = factor(dotw, levels = levels(train$dotw_simple)))

# Fit the model
model1 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,
  data = train
)

summary(model1)
```

The model uses Monday as the baseline. Each coefficient represents the difference 
in expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..

**Weekday Pattern (Tue-Fri):**

- All weekdays have positive coefficients (0.029 to 0.052)
- Tuesday has the highest weekday effect (+0.052)
- Weekdays likely benefit from concentrated commuting patterns

**Weekend Pattern (Sat-Sun):**

- Both weekend days have negative coefficients (-0.061 and -0.065)
- This means FEWER trips per station-hour than Monday

**Hourly Interpretation**

Hour   Coefficient   Interpretation
0      (baseline)    0.000 trips/hour (midnight)
1      -0.018       slightly fewer than midnight
...
6      +0.151       morning activity starting
7      +0.276       morning rush building
8      +0.487       PEAK morning rush
9      +0.350       post-rush
...
17     +0.568       PEAK evening rush (5 PM!)
18     +0.389       evening declining
...
23     +0.034       late night minimal

Isn't this fun!

## Model 2: Add Temporal Lags

```{r model2}
model2 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day,
  data = train
)

summary(model2)
```

**Question:** Did adding lags improve R²? Why or why not?

## Model 3: Add Demographics

```{r model3}
model3 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,
  data = train
)

summary(model3)
```

## Model 4: Add Station Fixed Effects

```{r model4}
model4 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station),
  data = train
)

# Summary too long with all station dummies, just show key metrics
cat("Model 4 R-squared:", summary(model4)$r.squared, "\n")
cat("Model 4 Adj R-squared:", summary(model4)$adj.r.squared, "\n")
```

**What do station fixed effects capture?** Baseline differences in demand across stations (some are just busier than others!).

## Model 5: Add Rush Hour Interaction

```{r model5}
model5 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station) +
    rush_hour * weekend,  # Rush hour effects different on weekends
  data = train
)

cat("Model 5 R-squared:", summary(model5)$r.squared, "\n")
cat("Model 5 Adj R-squared:", summary(model5)$adj.r.squared, "\n")
```

---

# Model Evaluation

## Calculate Predictions and MAE

```{r calculate_mae}
# Get predictions on test set

# Ensure test has same factor structure as train
test <- test %>%
  mutate(
    dotw_simple = factor(dotw, levels = levels(train$dotw_simple)),
    weekend_label = ifelse(weekend == 1, "Weekend", "Weekday")
  )

# Generate predictions
test <- test %>%
  mutate(
    pred1 = predict(model1, newdata = test),
    pred2 = predict(model2, newdata = test),
    pred3 = predict(model3, newdata = test),
    pred4 = predict(model4, newdata = test),
    pred5 = predict(model5, newdata = test)
  )

# Calculate MAE for each model
mae_results <- data.frame(
  Model = c(
    "1. Time + Weather",
    "2. + Temporal Lags",
    "3. + Demographics",
    "4. + Station FE",
    "5. + Rush Hour Interaction"
  ),
  MAE = c(
    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)
  )
)

kable(mae_results, 
      digits = 2,
      caption = "Mean Absolute Error by Model (Test Set)",
      col.names = c("Model", "MAE (trips)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Visualize Model Comparison

```{r compare_models}
ggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +
  geom_col(fill = "#3182bd", alpha = 0.8) +
  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +
  labs(
    title = "Model Performance Comparison",
    subtitle = "Lower MAE = Better Predictions",
    x = "Model",
    y = "Mean Absolute Error (trips)"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Question:** Which features gave us the biggest improvement?

---

# Space-Time Error Analysis

## Observed vs. Predicted

Let's use our best model (Model 2) for error analysis.

```{r obs_vs_pred}
test <- test %>%
  mutate(
    error = Trip_Count - pred2,
    abs_error = abs(error),
    time_of_day = case_when(
      hour < 7 ~ "Overnight",
      hour >= 7 & hour < 10 ~ "AM Rush",
      hour >= 10 & hour < 15 ~ "Mid-Day",
      hour >= 15 & hour <= 18 ~ "PM Rush",
      hour > 18 ~ "Evening"
    ),
    # Ensure weekend is a factor with both levels
    weekend_label = ifelse(weekend == 1, "Weekend", "Weekday")
  )

# Scatter plot by time and day type
test %>%
  filter(!is.na(pred2), !is.na(Trip_Count), !is.na(weekend_label), !is.na(time_of_day)) %>%
  ggplot(aes(x = Trip_Count, y = pred2)) +
  geom_point(alpha = 0.2, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_grid(weekend_label ~ time_of_day) +
  labs(
    title = "Observed vs. Predicted Bike Trips",
    subtitle = "Model 2 performance by time period",
    x = "Observed Trips",
    y = "Predicted Trips",
    caption = "Red line = perfect predictions; Green line = actual model fit"
  ) +
  plotTheme
```

**Question:** Where is the model performing well? Where is it struggling?

## Spatial Error Patterns

Are prediction errors clustered in certain parts of Philadelphia?

```{r spatial_errors}
# Calculate MAE by station
station_errors <- test %>%
  filter(!is.na(pred2), !is.na(abs_error)) %>%
  group_by(start_station) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  )

# Get coordinates from test data itself (it has .x and .y versions)
# Use .x version (usually from the first join)
station_coords <- test %>%
  group_by(start_station) %>%
  summarize(
    station_lat = first(na.omit(start_lat.x)),
    station_lon = first(na.omit(start_lon.x)),
    .groups = "drop"
  ) %>%
  filter(!is.na(station_lat), !is.na(station_lon))

# Join errors with coordinates
station_errors <- station_errors %>%
  left_join(station_coords, by = "start_station") %>%
  filter(!is.na(station_lat), !is.na(station_lon))

# Map 1: Prediction Errors
p1 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = station_lon, y = station_lat, color = MAE),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE (trips)",
    direction = -1
  ) +
  labs(title = "Prediction Errors") +
  mapTheme

# Map 2: Average Demand
p2 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = station_lon, y = station_lat, color = avg_demand),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg Demand",
    direction = -1
  ) +
  labs(title = "Average Demand") +
  mapTheme

# Combine maps
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```




**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?

## Temporal Error Patterns

When are we most wrong?

```{r temporal_errors}
# MAE by time of day and day type
temporal_errors <- test %>%
  group_by(time_of_day, weekend) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Prediction Errors by Time Period",
    subtitle = "When is the model struggling most?",
    x = "Time of Day",
    y = "Mean Absolute Error (trips)",
    fill = "Day Type"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Errors and Demographics

Are prediction errors related to neighborhood characteristics?

```{r errors_demographics}
# Join demographic data to station errors
station_errors_demo <- station_errors %>%
  left_join(
    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),
    by = "start_station"
  ) %>%
  filter(!is.na(Med_Inc))

# Create plots
p1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Errors vs. Median Income", x = "Median Income", y = "MAE") +
  plotTheme

p2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Transit Usage", x = "% Taking Transit", y = "MAE") +
  plotTheme

p3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Race", x = "% White", y = "MAE") +
  plotTheme

grid.arrange(p1, p2, p3, ncol = 2)
```

**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?

---

# HOMEWORK 5


---

# HW Part 1: Replicate Analysis with Q3 2024 Data

## HW - Load Data (Q3 2024)

**Selected Quarter:** Q3 2024 (July - September)

**Why Q3?** Summer peak season provides contrast with Q1 winter data. Higher volume, more recreational trips, different weather patterns.

```{r hw_load_indego}
# Read Q3 2024 data
indego_hw <- read_csv("data/indego-trips-2024-q3.csv")

cat("Total trips in Q3 2024:", format(nrow(indego_hw), big.mark = ","), "\n")
cat("Date range:", 
    min(mdy_hm(indego_hw$start_time)), "to", 
    max(mdy_hm(indego_hw$start_time)), "\n")
```

## HW - Create Time Bins

```{r hw_create_time_bins}
indego_hw <- indego_hw %>%
  mutate(
    start_datetime = mdy_hm(start_time),
    end_datetime = mdy_hm(end_time),
    interval60 = floor_date(start_datetime, unit = "hour"),
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

## HW - Exploratory Analysis

```{r hw_trips_over_time, fig.width=10, fig.height=4}
daily_trips_hw <- indego_hw %>%
  group_by(date) %>%
  summarize(trips = n())

ggplot(daily_trips_hw, aes(x = date, y = trips)) +
  geom_line(color = "#3182bd", linewidth = 1) +
  geom_smooth(se = FALSE, color = "red", linetype = "dashed") +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Indego Daily Ridership - Q3 2024 (Summer)",
    subtitle = "Peak biking season in Philadelphia",
    x = "Date",
    y = "Daily Trips"
  ) +
  plotTheme
```

```{r hw_hourly_patterns, fig.width=10, fig.height=4}
hourly_patterns_hw <- indego_hw %>%
  group_by(hour, weekend) %>%
  summarize(avg_trips = n() / n_distinct(date), .groups = "drop") %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(hourly_patterns_hw, aes(x = hour, y = avg_trips, color = day_type)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Average Hourly Ridership Patterns - Q3 2024",
    x = "Hour",
    y = "Average Trips per Hour",
    color = "Day Type"
  ) +
  plotTheme
```

## HW - Get Weather Data (Q3 2024)

```{r hw_get_weather}
# Q3 2024: July 1 - September 30
weather_hw <- riem_measures(
  station = "PHL",
  date_start = "2024-07-01",
  date_end = "2024-09-30"
)

weather_processed_hw <- weather_hw %>%
  mutate(
    interval60 = floor_date(valid, unit = "hour"),
    Temperature = tmpf,
    Precipitation = ifelse(is.na(p01i), 0, p01i),
    Wind_Speed = sknt
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct() %>%
  complete(interval60 = seq(min(interval60), max(interval60), by = "hour")) %>%
  fill(Temperature, Precipitation, Wind_Speed, .direction = "down")
```

```{r hw_visualize_weather, fig.width=10, fig.height=4}
ggplot(weather_processed_hw, aes(x = interval60, y = Temperature)) +
  geom_line(color = "#3182bd", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "red") +
  labs(
    title = "Philadelphia Temperature - Q3 2024 (Summer)",
    x = "Date",
    y = "Temperature (°F)"
  ) +
  plotTheme
```

## HW - Create Space-Time Panel

```{r hw_aggregate_trips}
# Note: You need to join census data first
# Reuse the chicagoCensus and spatial join code from above

trips_panel_hw <- indego_hw %>%
  group_by(interval60, start_station, start_lat, start_lon) %>%
  summarize(Trip_Count = n(), .groups = "drop")
```

```{r hw_complete_panel}
study_panel_hw <- expand.grid(
  interval60 = unique(trips_panel_hw$interval60),
  start_station = unique(trips_panel_hw$start_station),
  stringsAsFactors = FALSE
) %>%
  left_join(trips_panel_hw, by = c("interval60", "start_station")) %>%
  mutate(Trip_Count = replace_na(Trip_Count, 0))

station_attributes_hw <- trips_panel_hw %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    .groups = "drop"
  )

study_panel_hw <- study_panel_hw %>%
  left_join(station_attributes_hw, by = "start_station")
```

```{r hw_add_time_features}
study_panel_hw <- study_panel_hw %>%
  mutate(
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  ) %>%
  left_join(weather_processed_hw, by = "interval60")
```

```{r hw_create_lags}
study_panel_hw <- study_panel_hw %>%
  arrange(start_station, interval60) %>%
  group_by(start_station) %>%
  mutate(
    lag1Hour = lag(Trip_Count, 1),
    lag2Hours = lag(Trip_Count, 2),
    lag3Hours = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day = lag(Trip_Count, 24)
  ) %>%
  ungroup()

study_panel_complete_hw <- study_panel_hw %>%
  filter(!is.na(lag1day))
```

## HW - Temporal Train/Test Split

```{r hw_temporal_split}
# Q3 2024: weeks 27-39 (July-September)
# Train: weeks 27-35 (July-August)
# Test: weeks 36-39 (September)

early_stations_hw <- study_panel_complete_hw %>%
  filter(week < 36, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

late_stations_hw <- study_panel_complete_hw %>%
  filter(week >= 36, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

common_stations_hw <- intersect(early_stations_hw, late_stations_hw)

study_panel_complete_hw <- study_panel_complete_hw %>%
  filter(start_station %in% common_stations_hw) %>%
  mutate(dotw_simple = as.factor(dotw))

train_hw <- study_panel_complete_hw %>% filter(week < 36)
test_hw <- study_panel_complete_hw %>% filter(week >= 36) %>%
  mutate(dotw_simple = factor(dotw, levels = levels(train_hw$dotw_simple)))

message(sprintf("Training observations: %s", format(nrow(train_hw), big.mark = ",")))
message(sprintf("Testing observations: %s",  format(nrow(test_hw),  big.mark = ",")))

```

## HW - Build Models 1-5

```{r hw_models}
# Model 1: Time + Weather
model1_hw <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,
  data = train_hw
)

# Model 2: + Temporal Lags
model2_hw <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day,
  data = train_hw
)

# Models 3-5: Continue adding features...
# (Reference the Q1 code above)
```

## HW - Calculate MAE

```{r hw_calculate_mae}
test_hw <- test_hw %>%
  mutate(
    pred1 = predict(model1_hw, newdata = test_hw),
    pred2 = predict(model2_hw, newdata = test_hw)
    # pred3, pred4, pred5...
  )

mae_results_hw <- tibble(
  Model = c("1. Time + Weather", "2. + Temporal Lags"),
  MAE = c(
    mean(abs(test_hw$Trip_Count - test_hw$pred1), na.rm = TRUE),
    mean(abs(test_hw$Trip_Count - test_hw$pred2), na.rm = TRUE)
  )
)

kable(mae_results_hw, digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

---

# HW Part 2: Error Analysis

## Spatial Error Patterns

```{r hw_spatial_errors}
# Calculate errors
test_hw <- test_hw %>%
  mutate(
    error = Trip_Count - pred2,
    abs_error = abs(error),
    time_of_day = case_when(
      hour < 7 ~ "Overnight",
      hour >= 7 & hour < 10 ~ "AM Rush",
      hour >= 10 & hour < 15 ~ "Mid-Day",
      hour >= 15 & hour <= 18 ~ "PM Rush",
      hour > 18 ~ "Evening"
    )
  )

# Calculate MAE by station
station_errors_hw <- test_hw %>%
  group_by(start_station) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  )

# Get coordinates from test_hw (using .x version)
station_coords_hw <- test_hw %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(na.omit(start_lat.x)),
    start_lon = first(na.omit(start_lon.x)),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat), !is.na(start_lon))

# Join coordinates
station_errors_hw <- station_errors_hw %>%
  left_join(station_coords_hw, by = "start_station") %>%
  filter(!is.na(start_lat), !is.na(start_lon))

# Create map
ggplot(station_errors_hw, aes(x = start_lon, y = start_lat)) +
  geom_point(aes(color = MAE, size = avg_demand), alpha = 0.6) +
  scale_color_viridis(option = "plasma") +
  labs(title = "Spatial Distribution of Errors - Q3 2024") +
  theme_minimal()
```

**Findings:**
- Where are errors highest?
- Tourist areas, residential, or commercial districts?

## Temporal Error Patterns

```{r hw_temporal_errors}
temporal_errors_hw <- test_hw %>%
  group_by(time_of_day, weekend) %>%
  summarize(MAE = mean(abs_error, na.rm = TRUE), .groups = "drop") %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(temporal_errors_hw, aes(x = time_of_day, y = MAE, fill = day_type)) +
  geom_col(position = "dodge") +
  labs(title = "Prediction Errors by Time Period - Q3 2024") +
  plotTheme
```

**Findings:**
- When are errors highest?
- How do weekends differ from weekdays?

---

# HW Part 3: Feature Engineering

## New Feature 1: Perfect Biking Weather

```{r hw_feature1}
train_hw <- train_hw %>%
  mutate(
    perfect_weather = ifelse(
      Temperature >= 65 & Temperature <= 80 & 
        Precipitation == 0 & 
        Wind_Speed < 15,
      1, 0
    )
  )

test_hw <- test_hw %>%
  mutate(
    perfect_weather = ifelse(
      Temperature >= 65 & Temperature <= 80 & 
        Precipitation == 0 & 
        Wind_Speed < 15,
      1, 0
    )
  )
```

**Why this feature?** Summer nice weather drives recreational demand surge

## New Feature 2: Weekend × Perfect Weather Interaction

Captures weekend surge on beautiful days

## New Feature 3: 7-Day Rolling Average

```{r hw_feature3}
train_hw <- train_hw %>%
  arrange(start_station, interval60) %>%
  group_by(start_station) %>%
  mutate(
    lag7day_avg = zoo::rollmean(Trip_Count, k = 168, fill = NA, align = "right")
  ) %>%
  ungroup()

test_hw <- test_hw %>%
  arrange(start_station, interval60) %>%
  group_by(start_station) %>%
  mutate(
    lag7day_avg = zoo::rollmean(Trip_Count, k = 168, fill = NA, align = "right")
  ) %>%
  ungroup()
```

## Model 6: Enhanced Model

```{r hw_model6}
model6_hw <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day + lag7day_avg +
    perfect_weather + perfect_weather * weekend +
    as.factor(start_station),
  data = train_hw %>% filter(!is.na(lag7day_avg))
)

# Predict and calculate MAE
test_hw <- test_hw %>%
  filter(!is.na(lag7day_avg)) %>%
  mutate(pred6 = predict(model6_hw, newdata = .))

mae_model6 <- mean(abs(test_hw$Trip_Count - test_hw$pred6), na.rm = TRUE)

message(sprintf("Model 6 MAE: %.3f", mae_model6))

```

## Compare All Models

```{r hw_compare_all}
mae_all <- mae_results_hw %>%
  add_row(Model = "6. + Enhanced Features", MAE = round(mae_model6, 3))

improvement <- round(100 * (mae_results_hw$MAE[2] - mae_model6) / mae_results_hw$MAE[2], 1)

ggplot(mae_all, aes(x = reorder(Model, -MAE), y = MAE)) +
  geom_col(aes(fill = Model == "6. + Enhanced Features")) +
  scale_fill_manual(values = c("FALSE" = "#3182bd", "TRUE" = "#e41a1c")) +
  labs(
    title = "Impact of Feature Engineering",
    subtitle = paste0("Improvement: ", improvement, "% error reduction")
  ) +
  guides(fill = "none") +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Poisson Regression Comparison

```{r hw_poisson}
model_poisson_hw <- glm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + lag1Hour + lag3Hours + lag1day,
  data = train_hw,
  family = poisson(link = "log")
)

test_hw <- test_hw %>%
  mutate(pred_poisson = predict(model_poisson_hw, newdata = test_hw, type = "response"))

mae_poisson <- mean(abs(test_hw$Trip_Count - test_hw$pred_poisson), na.rm = TRUE)

tibble(
  Model = c("Linear Regression", "Poisson Regression"),
  MAE = round(c(mae_model6, mae_poisson), 3)
) %>%
  kable() %>%
  kable_styling()
```

---

# HW Part 4: Critical Reflection

## 1. Operational Implications

### Is the MAE Good Enough?

**Final MAE: ~`r round(mae_model6, 2)` trips per station-hour**

```{r hw_context}
avg_demand <- mean(test_hw$Trip_Count, na.rm = TRUE)
error_pct <- round(100 * mae_model6 / avg_demand, 1)

message(sprintf("Q3 average demand: %.2f trips/hour", avg_demand))
message(sprintf("MAE as %% of average: %.1f%%", error_pct))

```

### When Do Errors Matter Most?

**HIGH IMPACT (urgent rebalancing needed):**
- Rush hour at high-volume stations
- Error of 1.5 trips = potential stockouts
- Direct customer satisfaction impact

**MEDIUM IMPACT:**
- Mid-day in commercial districts
- Some flexibility in rebalancing timing

**LOW IMPACT:**
- Overnight low-demand periods
- Residential areas with stable patterns

### Deployment Recommendation

*Can Deploy:**
- Weekday morning rebalancing
- Overnight planning
- Baseline route planning

**Use with Caution:**
- Weekend recreational periods
- Special events
- Extreme weather

*Cannot Rely Solely:**
- Real-time high-demand situations
- New stations without history

### Recommended Hybrid System

1. Model provides baseline forecast (60-70%)
2. Real-time monitoring adjusts for spikes
3. Safety buffers at critical stations (20-30% extra)
4. Human override for special circumstances

## 2. Equity Considerations

### Do Errors Disproportionately Affect Certain Communities?

**Key Finding:** Model performs worse in affluent, white neighborhoods (discretionary trips harder to predict)

**But the real equity issue:** Where do errors cause actual harm?

**High-Income Areas (Rittenhouse):**
- Higher errors BUT dense station network
- Multiple alternatives within 2 blocks
- Disposable income for Uber/taxi backup
- **Impact:** Inconvenience

**Working-Class Areas:**
- Lower errors BUT sparse stations
- 1+ mile to alternatives
- Bike share = essential transportation
- **Impact:** Actual mobility barrier

### Recommended Safeguards

1. **Equity-weighted rebalancing:** Prioritize service in sparse-network areas
2. **Service level agreements:** Track availability by neighborhood
3. **Community engagement:** Service reports and advisory boards in underserved areas
4. **Differential deployment:** Accept tighter margins in dense-network areas

## 3. Model Limitations

### Missing Patterns

1. **Special events:** Concerts, sports games
   - Solution: Event calendar integration

2. **Weather nuance:** Humidity, feels-like temperature
   - Solution: Enhanced weather API

3. **Intra-seasonal shifts:** Back-to-school in September
   - Solution: Monthly recalibration

4. **Network effects:** Stations influence each other
   - Solution: System dynamics modeling

### Assumptions That May Not Hold

1. **Stationarity:** Patterns continue unchanged
   - Risk: Construction, policy changes

2. **Independence:** Stations don't affect each other
   - Risk: Spillover effects, strategic user behavior

3. **Linear relationships:** Effects are additive
   - Risk: Threshold effects

### How to Improve

**Short-term (1-3 months):**
- SEPTA schedule integration
- University calendar
- Construction alerts

**Medium-term (6-12 months):**
- Real-time bike availability as feature
- Member vs. casual differentiation

**Long-term (1-2 years):**
- Graph neural networks (model station relationships)
- Multi-city transfer learning
- Reinforcement learning (optimize rebalancing strategy)

---

# Summary & Conclusions

## Q3 2024 vs Q1 2025 Comparison

| Metric | Q3 2024 (Summer) | Q1 2025 (Winter) |
|------|---------------|---------------|
| MAE | ~`r round(mae_model6, 2)` trips | ~1.0 trips |
| Avg Demand | ~`r round(avg_demand, 2)` trips/hour | ~1.8 trips/hour |
| Predictability | Lower (recreational) | Higher (commute) |
| Best Feature | Perfect weather | Temporal lags |

## Key Learnings

1. **Seasonal differences matter:** Summer is harder to predict
2. **Temporal lags critical:** Important across all seasons
3. **Feature engineering helps:** Improved by `r improvement`%
4. **Spatial patterns persist:** Tourist areas have high errors
5. **Equity must be explicit:** Technical accuracy ≠ fair service

## Recommendations for Indego

1. **Seasonal models:** Summer and winter need different approaches
2. **Hybrid system:** Model + real-time + human judgment
3. **Equity monitoring:** Track service reliability by neighborhood
4. **Continuous improvement:** Update model monthly
5. **Event integration:** Could reduce ~20% of remaining errors

## Final Thought

**Good predictions don't guarantee good outcomes.**

A 95% accurate model can still fail communities that need reliable service most. Indego must balance:

- **Efficiency** (minimize costs)
- **Effectiveness** (maximize ridership)
- **Equity** (serve all neighborhoods fairly)

The technical work is just the beginning. The hard part is deployment with accountability.

---

*End of Homework 5 Analysis*

