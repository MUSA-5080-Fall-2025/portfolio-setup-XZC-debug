poverty_rate = "B17001_002"     # Population in poverty
),
year = 2022,
output = "wide"
)
#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)
census_api_key("547c6d8007c794850c2b25b34f1ce85befe8ab09")
#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)
census_api_key("547c6d8007c794850c2b25b34f1ce85befe8ab09")
#| echo: false
#| eval: false
library(tidyverse)
# Generate data following Y = f(X) + ε
set.seed(789)
n <- 50
x <- seq(0, 10, length.out = n)
# True function f(X) - let's make it slightly curved
f_x <- 5 + 2*x - 0.1*x^2
# Add random error ε
epsilon <- rnorm(n, 0, 2)
y <- f_x + epsilon
# Create data frame
data <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)
# Highlight a few specific points to show the decomposition
highlight_points <- c(10, 25, 40)
data$highlight <- 1:n %in% highlight_points
# Create the visualization
ggplot(data, aes(x = x)) +
geom_line(aes(y = f_x), color = "#2C3E50", linewidth = 1.5) +
geom_point(aes(y = y, size = highlight, alpha = highlight),
color = "#E74C3C", show.legend = FALSE) +
geom_segment(data = data[data$highlight, ],
aes(x = x, xend = x, y = f_x, yend = y),
color = "#9B59B6", linewidth = 1,
arrow = arrow(length = unit(0.2, "cm"), ends = "both")) +
annotate("text", x = 2, y = 22, label = "Y = f(X) + ε",
size = 6, fontface = "bold", hjust = 0) +
annotate("text", x = 8, y = 18, label = "f(X)\n(systematic)",
size = 5, color = "#2C3E50", hjust = 0) +
annotate("text", x = 5.5, y = 10, label = "ε\n(random error)",
size = 4, color = "#9B59B6") +
annotate("point", x = 1, y = 8, size = 4, color = "#E74C3C") +
annotate("text", x = 1.5, y = 8, label = "Observed Y",
size = 4, color = "#E74C3C", hjust = 0) +
scale_size_manual(values = c(2, 4)) +
scale_alpha_manual(values = c(0.6, 1)) +
labs(x = "X (predictors)", y = "Y (outcome)",
title = "The Statistical Learning Framework",
subtitle = "Our goal: Estimate f(X) from observed data") +
theme_minimal(base_size = 14) +
theme(plot.title = element_text(face = "bold", size = 16),
panel.grid.minor = element_blank())
ggsave("images/statistical_learning_framework.png",
width = 10, height = 6, dpi = 300)
#| echo: false
#| eval: true
# Fetch PA county data directly from Census API
pa_data <- get_acs(
geography = "county",
state = "PA",
variables = c(
total_pop = "B01003_001",
median_income = "B19013_001"
),
year = 2022,
output = "wide"
)
# Visualize the relationship
ggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +
geom_point(alpha = 0.6, size = 3) +
geom_smooth(method = "lm", se = TRUE, color = "steelblue") +
labs(
title = "Population vs Median Income in PA Counties",
x = "Total Population",
y = "Median Household Income"
) +
scale_x_continuous(labels = comma) +
scale_y_continuous(labels = dollar) +
theme_minimal()
#| echo: true
#| eval: true
model1 <- lm(median_incomeE ~ total_popE, data = pa_data)
summary(model1)
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4
# Create example data with clear pattern
set.seed(123)
x <- seq(0, 10, 0.5)
y <- 2 + 0.5*x + rnorm(length(x), 0, 1)
example_data <- data.frame(x = x, y = y)
# Three models
p1 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
labs(title = "Underfitting", subtitle = "Ignores relationship") +
theme_minimal()
p2 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Good Fit", subtitle = "Captures true pattern") +
theme_minimal()
p3 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ poly(x, 10), se = FALSE) +
labs(title = "Overfitting", subtitle = "Follows noise") +
theme_minimal()
library(patchwork)
p1 | p2 | p3
#| echo: true
#| eval: true
set.seed(123)
n <- nrow(pa_data)
# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- pa_data[train_indices, ]
test_data <- pa_data[-train_indices, ]
# Fit on training data only
model_train <- lm(median_incomeE ~ total_popE, data = train_data)
# Predict on test data
test_predictions <- predict(model_train, newdata = test_data)
#| echo: true
#| eval: true
# Calculate prediction error (RMSE)
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma
cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
#| echo: true
#| eval: true
library(caret)
#| echo: true
#| eval: true
# Calculate prediction error (RMSE)
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma
cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
#| echo: true
#| eval: true
library(caret)
#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)
census_api_key("547c6d8007c794850c2b25b34f1ce85befe8ab09", install = TRUE)
#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)
#census_api_key("547c6d8007c794850c2b25b34f1ce85befe8ab09", install = TRUE)
#| echo: false
#| eval: false
library(tidyverse)
# Generate data following Y = f(X) + ε
set.seed(789)
n <- 50
x <- seq(0, 10, length.out = n)
# True function f(X) - let's make it slightly curved
f_x <- 5 + 2*x - 0.1*x^2
# Add random error ε
epsilon <- rnorm(n, 0, 2)
y <- f_x + epsilon
# Create data frame
data <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)
# Highlight a few specific points to show the decomposition
highlight_points <- c(10, 25, 40)
data$highlight <- 1:n %in% highlight_points
# Create the visualization
ggplot(data, aes(x = x)) +
geom_line(aes(y = f_x), color = "#2C3E50", linewidth = 1.5) +
geom_point(aes(y = y, size = highlight, alpha = highlight),
color = "#E74C3C", show.legend = FALSE) +
geom_segment(data = data[data$highlight, ],
aes(x = x, xend = x, y = f_x, yend = y),
color = "#9B59B6", linewidth = 1,
arrow = arrow(length = unit(0.2, "cm"), ends = "both")) +
annotate("text", x = 2, y = 22, label = "Y = f(X) + ε",
size = 6, fontface = "bold", hjust = 0) +
annotate("text", x = 8, y = 18, label = "f(X)\n(systematic)",
size = 5, color = "#2C3E50", hjust = 0) +
annotate("text", x = 5.5, y = 10, label = "ε\n(random error)",
size = 4, color = "#9B59B6") +
annotate("point", x = 1, y = 8, size = 4, color = "#E74C3C") +
annotate("text", x = 1.5, y = 8, label = "Observed Y",
size = 4, color = "#E74C3C", hjust = 0) +
scale_size_manual(values = c(2, 4)) +
scale_alpha_manual(values = c(0.6, 1)) +
labs(x = "X (predictors)", y = "Y (outcome)",
title = "The Statistical Learning Framework",
subtitle = "Our goal: Estimate f(X) from observed data") +
theme_minimal(base_size = 14) +
theme(plot.title = element_text(face = "bold", size = 16),
panel.grid.minor = element_blank())
ggsave("images/statistical_learning_framework.png",
width = 10, height = 6, dpi = 300)
#| echo: false
#| eval: true
# Fetch PA county data directly from Census API
pa_data <- get_acs(
geography = "county",
state = "PA",
variables = c(
total_pop = "B01003_001",
median_income = "B19013_001"
),
year = 2022,
output = "wide"
)
# Visualize the relationship
ggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +
geom_point(alpha = 0.6, size = 3) +
geom_smooth(method = "lm", se = TRUE, color = "steelblue") +
labs(
title = "Population vs Median Income in PA Counties",
x = "Total Population",
y = "Median Household Income"
) +
scale_x_continuous(labels = comma) +
scale_y_continuous(labels = dollar) +
theme_minimal()
#| echo: true
#| eval: true
model1 <- lm(median_incomeE ~ total_popE, data = pa_data)
summary(model1)
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4
# Create example data with clear pattern
set.seed(123)
x <- seq(0, 10, 0.5)
y <- 2 + 0.5*x + rnorm(length(x), 0, 1)
example_data <- data.frame(x = x, y = y)
# Three models
p1 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
labs(title = "Underfitting", subtitle = "Ignores relationship") +
theme_minimal()
p2 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Good Fit", subtitle = "Captures true pattern") +
theme_minimal()
p3 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ poly(x, 10), se = FALSE) +
labs(title = "Overfitting", subtitle = "Follows noise") +
theme_minimal()
library(patchwork)
p1 | p2 | p3
#| echo: true
#| eval: true
set.seed(123)
n <- nrow(pa_data)
# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- pa_data[train_indices, ]
test_data <- pa_data[-train_indices, ]
# Fit on training data only
model_train <- lm(median_incomeE ~ total_popE, data = train_data)
# Predict on test data
test_predictions <- predict(model_train, newdata = test_data)
#| echo: true
#| eval: true
# Calculate prediction error (RMSE)
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma
cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
#| echo: true
#| eval: true
library(caret)
install.packages("caret")
#| echo: true
#| eval: true
library(caret)
# 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(median_incomeE ~ total_popE,
data = pa_data,
method = "lm",
trControl = train_control)
cv_model$results
#| echo: true
#| eval: true
pa_data$residuals <- residuals(model1)
pa_data$fitted <- fitted(model1)
ggplot(pa_data, aes(x = fitted, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals") +
theme_minimal()
#| echo: true
#| eval: true
library(lmtest)
install.packages("lmtest")
#| echo: true
#| eval: true
library(lmtest)
bptest(model1)
#| echo: false
#| eval: true
#| fig-width: 5
#| fig-height: 4
q <- ggplot(pa_data, aes(sample = residuals)) +
stat_qq() +
stat_qq_line(color = "red") +
labs(title = "Q-Q Plot of Residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles") +
theme_minimal()
print(q)
#| echo: true
#| eval: false
library(car)
install.packages("car")
#| echo: true
#| eval: false
library(car)
vif(model1)  # Variance Inflation Factor
#| echo: false
#| eval: true
#| fig-width: 5
#| fig-height: 4
# Add diagnostic measures
pa_data <- pa_data %>%
mutate(
cooks_d = cooks.distance(model1),
leverage = hatvalues(model1),
is_influential = cooks_d > 4/nrow(pa_data)
)
# Plot Cook's distance
ggplot(pa_data, aes(x = 1:nrow(pa_data), y = cooks_d)) +
geom_point(aes(color = is_influential), size = 2) +
geom_hline(yintercept = 4/nrow(pa_data),
linetype = "dashed", color = "red") +
scale_color_manual(values = c("grey60", "red")) +
labs(title = "Cook's Distance",
x = "Observation", y = "Cook's D") +
theme_minimal() +
theme(legend.position = "none")
#| echo: false
#| eval: true
# Rule of thumb: Cook's D > 4/n
threshold <- 4/nrow(pa_data)
influential <- pa_data %>%
filter(cooks_d > threshold) %>%
select(NAME, total_popE, median_incomeE, cooks_d) %>%
arrange(desc(cooks_d))
head(influential, 2)
#| echo: false
#| eval: true
# Get more variables
pa_data_full <- get_acs(
geography = "county",
state = "PA",
variables = c(
total_pop = "B01003_001",
median_income = "B19013_001",
percent_college = "B15003_022",
poverty_rate = "B17001_002"
),
year = 2022,
output = "wide"
)
# Multiple regression
model2 <- lm(median_incomeE ~ total_popE + percent_collegeE + poverty_rateE,
data = pa_data_full)
summary(model2)
#| echo: true
#| eval: true
# Compare linear vs log
model_linear <- lm(median_incomeE ~ total_popE, data = pa_data)
model_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)
summary(model_log)
# Check which residual plot looks better
#| echo: true
#| eval: true
# Create metro/non-metro indicator
pa_data <- pa_data %>%
mutate(metro = ifelse(total_popE > 500000, 1, 0))
model3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)
summary(model3)
#| eval: false
#| echo: true
challenge_data <- get_acs(
geography = "county",
state = "PA",
variables = c(
home_value = "B25077_001",      # YOUR TARGET
total_pop = "B01003_001",       # Total population
median_income = "B19013_001",   # Median household income
median_age = "B01002_001",      # Median age
percent_college = "B15003_022", # Bachelor's degree or higher
median_rent = "B25058_001",     # Median rent
poverty_rate = "B17001_002"     # Population in poverty
),
year = 2022,
output = "wide"
)
library(caret)
# 确保数据干净
data_single <- na.omit(challenge_data)
# 定义 10 折交叉验证控制
ctrl <- trainControl(method = "cv", number = 10)
# 自变量名称列表（除了目标变量）
predictors <- c("total_popE", "median_incomeE", "median_ageE",
"percent_collegeE", "median_rentE", "poverty_rateE")
# 用循环或lapply对每个predictor建模并提取RMSE
rmse_list <- lapply(predictors, function(var){
f <- as.formula(paste("home_valueE ~", var))
model <- train(f, data = data_single,
method = "lm",
trControl = ctrl,
metric = "RMSE")
tibble(predictor = var, RMSE = model$results$RMSE)
})
library(dplyr)
library(tidyr)
library(caret)
# 假设你已有 challenge_data（来自 get_acs(output="wide")）
# 1) 只保留要参与单变量比较的列（Estimate 列，以 E 结尾）
df_single <- challenge_data %>%
dplyr::select(
home_valueE,       # 目标
total_popE,
median_incomeE,
median_ageE,
percent_collegeE,
median_rentE,
poverty_rateE
) %>%
tidyr::drop_na()     # 仅对这些列去 NA
# （可选）检查每列 NA 数
sapply(df_single, function(x) sum(is.na(x)))
nrow(df_single)  # 确认还有有效行
library(dplyr)
library(tidyr)
library(caret)
# 假设你已有 challenge_data（来自 get_acs(output="wide")）
# 1) 只保留要参与单变量比较的列（Estimate 列，以 E 结尾）
df_single <- challenge_data %>%
dplyr::select(
home_valueE,       # 目标
total_popE,
median_incomeE,
median_ageE,
percent_collegeE,
median_rentE,
poverty_rateE
) %>%
tidyr::drop_na()     # 仅对这些列去 NA
# （可选）检查每列 NA 数
sapply(df_single, function(x) sum(is.na(x)))
nrow(df_single)  # 确认还有有效行
set.seed(5080)
ctrl <- trainControl(method = "cv", number = 10)
predictors <- c("total_popE", "median_incomeE", "median_ageE",
"percent_collegeE", "median_rentE", "poverty_rateE")
rmse_list <- lapply(predictors, function(var){
f <- as.formula(paste("home_valueE ~", var))
model <- train(
f, data = df_single,
method = "lm",
trControl = ctrl,
metric = "RMSE",
na.action = na.omit   # 保险：再声明一次
)
tibble(predictor = var, RMSE = model$results$RMSE)
})
rmse_df <- dplyr::bind_rows(rmse_list) %>% arrange(RMSE)
print(rmse_df)
set.seed(5080)
ctrl <- trainControl(method = "cv", number = 10)
predictors <- c("total_popE", "median_incomeE", "median_ageE",
"percent_collegeE", "median_rentE", "poverty_rateE")
rmse_list <- lapply(predictors, function(var){
f <- as.formula(paste("home_valueE ~", var))
model <- train(
f, data = df_single,
method = "lm",
trControl = ctrl,
metric = "RMSE",
na.action = na.omit
)
tibble(predictor = var, RMSE = model$results$RMSE)
})
rmse_df <- dplyr::bind_rows(rmse_list) %>% arrange(RMSE)
print(rmse_df)
library(ggplot2)
ggplot(df_single, aes(x = median_incomeE, y = home_valueE)) +
geom_point(color = "steelblue", size = 2) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Median Home Value vs Median Income (PA Counties)",
x = "Median Household Income ($)", y = "Median Home Value ($)")
