"exterior_condition","interior_condition",
"park_within_15min_walk","transit_15min_walk",
"hospitals_15min_walk","schools_within_15min_walk",
"sale_year","sale_quarter"
)
target <- "log_sale_price"
model_data <- df_fe |>
select(all_of(c(cont_vars, other_vars, target))) |>
drop_na()
# Step 4 — Assemble modeling dataset and scale continuous variables (updated)
# continuous variables to scale (use _log names from Step 3; keep crime_count raw)
cont_vars <- c(
"number_of_bedrooms","number_of_bathrooms","total_livable_area","house_age",
"median_incomeE","per_cap_incomeE","PCTPOVERTY","PCBACHMORE",
"dist_to_park_ft_log","dist_transit_ft_log","dist_to_hospital_ft_log",
"dist_to_nearest_school_ft_log","crime_count_15min_walk"
)
# other predictors (not scaled)
other_vars <- c(
"exterior_condition","interior_condition",
"park_within_15min_walk","transit_15min_walk",
"hospitals_15min_walk","schools_within_15min_walk",
"sale_year","sale_quarter"
)
target <- "log_sale_price"
# select only existing columns (avoids errors if any optional col is missing)
all_needed <- c(cont_vars, other_vars, target)
missing_cols <- setdiff(all_needed, names(df_fe))
if (length(missing_cols) > 0) {
warning("These columns are missing and will be skipped: ",
paste(missing_cols, collapse = ", "))
all_needed <- setdiff(all_needed, missing_cols)
cont_vars  <- setdiff(cont_vars,  missing_cols)
other_vars <- setdiff(other_vars, missing_cols)
}
model_data <- df_fe |>
dplyr::select(dplyr::all_of(all_needed)) |>
tidyr::drop_na()
# scale continuous variables
model_data <- model_data |>
dplyr::mutate(dplyr::across(dplyr::all_of(cont_vars), ~ as.numeric(scale(.x))))
# preview dataset
dim(model_data)
summary(model_data[[target]])
model_data |> dplyr::slice_head(n = 5)
#| message: false
#| warning: false
# Step 1 — Load packages and data, basic checks
library(tidyverse)
library(lubridate)
# load dataset (adjust path if needed)
df <- readr::read_csv("data2/opa_sales_final_complete.csv", show_col_types = FALSE)
# basic info
dim(df)
glimpse(df)
colSums(is.na(df)) |> sort(decreasing = TRUE) |> head(10)
# Step 2 — Target variable inspection (sale_price vs. log)
df <- df |>
mutate(
log_sale_price = log(sale_price)
)
# distribution plots
p1 <- ggplot(df, aes(sale_price)) +
geom_histogram(bins = 50, fill = "steelblue", color = "white") +
labs(title = "Distribution of Sale Price", x = "Sale Price ($)", y = "Count")
p2 <- ggplot(df, aes(log_sale_price)) +
geom_histogram(bins = 50, fill = "seagreen", color = "white") +
labs(title = "Distribution of log(Sale Price)", x = "log(Sale Price)", y = "Count")
p1
p2
# simple skewness check without external package
skewness <- function(x) {
x <- x[!is.na(x)]
n <- length(x)
m3 <- mean((x - mean(x))^3)
m2 <- mean((x - mean(x))^2)
(n * m3) / ((n - 1) * (n - 2) * (m2^(3/2)))
}
skew_raw <- skewness(df$sale_price)
skew_log <- skewness(df$log_sale_price)
tibble(skew_raw, skew_log)
# Check distribution of crime_count_15min_walk before and after log1p
p_raw <- ggplot(df, aes(x = crime_count_15min_walk)) +
geom_histogram(bins = 50, fill = "salmon", color = "white") +
labs(
title = "Distribution of crime_count_15min_walk (Raw)",
x = "Crime count within 15-min walk",
y = "Frequency"
) +
theme_minimal()
p_log <- ggplot(df_fe, aes(x = log_crime_count_15min_walk)) +
geom_histogram(bins = 50, fill = "steelblue", color = "white") +
labs(
title = "Distribution of log1p(crime_count_15min_walk)",
x = "log1p(Crime count)",
y = "Frequency"
) +
theme_minimal()
p_raw
p_log
#| message: false
#| warning: false
# Step 1 — Load packages and data, basic checks
library(tidyverse)
library(lubridate)
# load dataset (adjust path if needed)
df <- readr::read_csv("data2/opa_sales_final_complete.csv", show_col_types = FALSE)
# basic info
dim(df)
glimpse(df)
colSums(is.na(df)) |> sort(decreasing = TRUE) |> head(10)
# Step 2 — Target variable inspection (sale_price vs. log)
df <- df |>
mutate(
log_sale_price = log(sale_price)
)
# distribution plots
p1 <- ggplot(df, aes(sale_price)) +
geom_histogram(bins = 50, fill = "steelblue", color = "white") +
labs(title = "Distribution of Sale Price", x = "Sale Price ($)", y = "Count")
p2 <- ggplot(df, aes(log_sale_price)) +
geom_histogram(bins = 50, fill = "seagreen", color = "white") +
labs(title = "Distribution of log(Sale Price)", x = "log(Sale Price)", y = "Count")
p1
p2
# simple skewness check without external package
skewness <- function(x) {
x <- x[!is.na(x)]
n <- length(x)
m3 <- mean((x - mean(x))^3)
m2 <- mean((x - mean(x))^2)
(n * m3) / ((n - 1) * (n - 2) * (m2^(3/2)))
}
skew_raw <- skewness(df$sale_price)
skew_log <- skewness(df$log_sale_price)
tibble(skew_raw, skew_log)
# Step 3 — Feature engineering (updated, no log transform for crime)
df_fe <- df |>
mutate(
# house age
house_age = 2025 - year_built,
# time features
sale_date = suppressWarnings(lubridate::ymd(sale_date)),
sale_year = year(sale_date),
sale_quarter = quarter(sale_date),
# log1p transforms for only highly skewed distance variables
dist_to_park_ft_log          = log1p(dist_to_park_ft),
dist_transit_ft_log          = log1p(dist_transit_ft),
dist_to_hospital_ft_log      = log1p(dist_to_hospital_ft),
dist_to_nearest_school_ft_log= log1p(dist_to_nearest_school_ft)
)
glimpse(df_fe)
# Step 4 — Assemble modeling dataset and scale continuous variables (updated)
# continuous variables to scale (use _log names from Step 3; keep crime_count raw)
cont_vars <- c(
"number_of_bedrooms","number_of_bathrooms","total_livable_area","house_age",
"median_incomeE","per_cap_incomeE","PCTPOVERTY","PCBACHMORE",
"dist_to_park_ft_log","dist_transit_ft_log","dist_to_hospital_ft_log",
"dist_to_nearest_school_ft_log","crime_count_15min_walk"
)
# other predictors (not scaled)
other_vars <- c(
"exterior_condition","interior_condition",
"park_within_15min_walk","transit_15min_walk",
"hospitals_15min_walk","schools_within_15min_walk",
"sale_year","sale_quarter"
)
target <- "log_sale_price"
# select only existing columns (avoids errors if any optional col is missing)
all_needed <- c(cont_vars, other_vars, target)
missing_cols <- setdiff(all_needed, names(df_fe))
if (length(missing_cols) > 0) {
warning("These columns are missing and will be skipped: ",
paste(missing_cols, collapse = ", "))
all_needed <- setdiff(all_needed, missing_cols)
cont_vars  <- setdiff(cont_vars,  missing_cols)
other_vars <- setdiff(other_vars, missing_cols)
}
model_data <- df_fe |>
dplyr::select(dplyr::all_of(all_needed)) |>
tidyr::drop_na()
# scale continuous variables
model_data <- model_data |>
dplyr::mutate(dplyr::across(dplyr::all_of(cont_vars), ~ as.numeric(scale(.x))))
# preview dataset
dim(model_data)
summary(model_data[[target]])
model_data |> dplyr::slice_head(n = 5)
# Step 5 — Baseline linear regression model (OLS)
library(broom)
library(car)
# build OLS model
model_ols <- lm(log_sale_price ~ ., data = model_data)
# model summary
summary(model_ols)
# Variance Inflation Factor (VIF) check for multicollinearity
vif_vals <- car::vif(model_ols)
sort(vif_vals, decreasing = TRUE)[1:10]
# extract tidy table of coefficients
tidy(model_ols) |>
arrange(p.value) |>
head(10)
# Step 5b — Residual diagnostics
par(mfrow = c(2, 2))
plot(model_ols)
# Shapiro–Wilk test for normality of residuals
# Residuals
resid_ols <- residuals(model_ols)
# Shapiro–Wilk on a random subsample of size 5000
set.seed(5080)
resid_sub <- sample(resid_ols, size = 5000, replace = FALSE)
shapiro.test(resid_sub)
# Breusch–Pagan test for heteroskedasticity
library(lmtest)
bptest(model_ols)
# Step 6 — Regularization (Lasso / Ridge)
library(glmnet)
library(caret)
library(dplyr)
# prepare matrices
X <- model_data |> select(-log_sale_price) |> as.matrix()
y <- model_data$log_sale_price
set.seed(5080)
cv_lasso <- cv.glmnet(X, y, alpha = 1, nfolds = 10)
cv_ridge <- cv.glmnet(X, y, alpha = 0, nfolds = 10)
# plot CV errors
par(mfrow = c(1, 2))
plot(cv_lasso, main = "Lasso 10-fold CV")
plot(cv_ridge, main = "Ridge 10-fold CV")
par(mfrow = c(1, 1))
# best lambda and test error
tibble(
Model = c("Lasso", "Ridge"),
Lambda_min = c(cv_lasso$lambda.min, cv_ridge$lambda.min),
Lambda_1se = c(cv_lasso$lambda.1se, cv_ridge$lambda.1se),
CV_MSE = c(min(cv_lasso$cvm), min(cv_ridge$cvm))
)
# Step 6b — Extract coefficients and variable importance
coef_lasso <- coef(cv_lasso, s = "lambda.min") |> as.matrix()
coef_ridge <- coef(cv_ridge, s = "lambda.min") |> as.matrix()
# top absolute coefficients (Lasso)
lasso_imp <- tibble(
feature = rownames(coef_lasso),
coef = as.numeric(coef_lasso)
) |>
filter(feature != "(Intercept)") |>
arrange(desc(abs(coef))) |>
slice_head(n = 10)
lasso_imp
# Step 7 — Train/test split and metrics helpers
set.seed(5080)
n <- nrow(model_data)
idx_train <- sample.int(n, size = floor(0.8 * n))
train <- model_data[idx_train, ]
test  <- model_data[-idx_train, ]
rmse <- function(actual, pred) sqrt(mean((actual - pred)^2))
mae  <- function(actual, pred) mean(abs(actual - pred))
r2   <- function(actual, pred) 1 - sum((actual - pred)^2) / sum((actual - mean(actual))^2)
# Step 7a — Random Forest (ranger)
if (!requireNamespace("ranger", quietly = TRUE)) install.packages("ranger")
library(ranger)
# build RF with OOB error; tune a few core hyperparameters
rf_fit <- ranger(
formula         = log_sale_price ~ .,
data            = train,
num.trees       = 1000,
mtry            = floor(sqrt(ncol(train) - 1)),
min.node.size   = 5,
respect.unordered.factors = "order",
importance      = "impurity",
seed            = 5080
)
# OOB RMSE (training proxy)
rf_oob_rmse <- sqrt(rf_fit$prediction.error)
# Test predictions and metrics
rf_pred <- predict(rf_fit, data = test)$predictions
rf_rmse <- rmse(test$log_sale_price, rf_pred)
rf_mae  <- mae(test$log_sale_price, rf_pred)
rf_r2   <- r2(test$log_sale_price, rf_pred)
data.frame(
Model = "Random Forest",
OOB_RMSE = rf_oob_rmse,
Test_RMSE = rf_rmse,
Test_MAE = rf_mae,
Test_R2 = rf_r2
)
# Step 7a.1 — RF variable importance (top 15)
rf_imp <- as.data.frame(rf_fit$variable.importance)
rf_imp$feature <- rownames(rf_imp)
colnames(rf_imp)[1] <- "importance"
rf_imp <- rf_imp[order(rf_imp$importance, decreasing = TRUE), ]
head(rf_imp, 15)
# Step 7b — XGBoost (with CV to pick nrounds)
if (!requireNamespace("xgboost", quietly = TRUE)) install.packages("xgboost")
if (!requireNamespace("Matrix", quietly = TRUE))  install.packages("Matrix")
library(xgboost)
library(Matrix)
# build sparse design matrices (drop target)
x_train <- as.matrix(train[, !(names(train) %in% "log_sale_price")])
x_test  <- as.matrix(test[,  !(names(test)  %in% "log_sale_price")])
y_train <- train$log_sale_price
y_test  <- test$log_sale_price
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test,  label = y_test)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.05,
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 5
)
set.seed(5080)
cv_res <- xgb.cv(
params = params,
data = dtrain,
nrounds = 3000,
nfold = 5,
early_stopping_rounds = 50,
verbose = 0
)
best_nrounds <- cv_res$best_iteration
xgb_fit <- xgb.train(
params = params,
data = dtrain,
nrounds = best_nrounds,
watchlist = list(train = dtrain),
verbose = 0
)
xgb_pred <- predict(xgb_fit, dtest)
xgb_rmse <- rmse(y_test, xgb_pred)
xgb_mae  <- mae(y_test, xgb_pred)
xgb_r2   <- r2(y_test, xgb_pred)
data.frame(
Model = "XGBoost",
Best_nrounds = best_nrounds,
Test_RMSE = xgb_rmse,
Test_MAE = xgb_mae,
Test_R2 = xgb_r2
)
xgb_imp <- xgb.importance(model = xgb_fit)
head(xgb_imp, 15)
# Phase 5 — 10-fold Cross-Validation across 4 models
library(caret)
library(dplyr)
library(ggplot2)
# Ensure factors for fixed effects
model_data_cv <- model_data %>%
mutate(
sale_year = factor(sale_year),
sale_quarter = factor(sale_quarter)
)
# Define progressive feature sets
vars_structural <- c(
"number_of_bedrooms","number_of_bathrooms","total_livable_area",
"house_age","exterior_condition","interior_condition"
)
vars_census <- c(
"median_incomeE","per_cap_incomeE","PCTPOVERTY","PCBACHMORE"
)
vars_spatial <- c(
"dist_to_park_ft_log","dist_transit_ft_log","dist_to_hospital_ft_log",
"dist_to_nearest_school_ft_log","crime_count_15min_walk",
"park_within_15min_walk","transit_15min_walk","hospitals_15min_walk","schools_within_15min_walk"
)
# Build model formulas
form_structural <- as.formula(
paste("log_sale_price ~", paste(vars_structural, collapse = " + "))
)
form_struct_census <- as.formula(
paste("log_sale_price ~", paste(c(vars_structural, vars_census), collapse = " + "))
)
form_struct_census_spatial <- as.formula(
paste("log_sale_price ~", paste(c(vars_structural, vars_census, vars_spatial), collapse = " + "))
)
# Interactions & fixed effects (examples: bedrooms*bathrooms; area with crime; year/quarter FE)
form_inter_fe <- as.formula(
paste(
"log_sale_price ~",
paste(c(vars_structural, vars_census, vars_spatial), collapse = " + "),
"+ number_of_bedrooms:number_of_bathrooms",
"+ total_livable_area:crime_count_15min_walk",
"+ sale_year + sale_quarter"
)
)
# Custom summary to add MAE
maeSummary <- function(data, lev = NULL, model = NULL) {
out <- c(
RMSE = sqrt(mean((data$obs - data$pred)^2)),
Rsquared = cor(data$obs, data$pred)^2,
MAE = mean(abs(data$obs - data$pred))
)
out
}
set.seed(5080)
ctrl <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
summaryFunction = maeSummary
)
# Train four models with 10-fold CV (linear baseline for fair comparison)
set.seed(5080)
fit_structural <- train(
form_structural, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)
set.seed(5080)
fit_struct_census <- train(
form_struct_census, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)
set.seed(5080)
fit_struct_census_spatial <- train(
form_struct_census_spatial, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)
set.seed(5080)
fit_inter_fe <- train(
form_inter_fe, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)
# Collect CV results (RMSE, R^2, MAE)
cv_table <- bind_rows(
tibble(Model = "Structural Only",
RMSE = fit_structural$results$RMSE[1],
Rsq  = fit_structural$results$Rsquared[1],
MAE  = fit_structural$results$MAE[1]),
tibble(Model = "+ Census",
RMSE = fit_struct_census$results$RMSE[1],
Rsq  = fit_struct_census$results$Rsquared[1],
MAE  = fit_struct_census$results$MAE[1]),
tibble(Model = "+ Spatial",
RMSE = fit_struct_census_spatial$results$RMSE[1],
Rsq  = fit_struct_census_spatial$results$Rsquared[1],
MAE  = fit_struct_census_spatial$results$MAE[1]),
tibble(Model = "+ Interactions / FE",
RMSE = fit_inter_fe$results$RMSE[1],
Rsq  = fit_inter_fe$results$Rsquared[1],
MAE  = fit_inter_fe$results$MAE[1])
) %>%
arrange(RMSE)
cv_table
# Predicted vs Actual (OOF) for the best model by RMSE (usually Interactions/FE)
best_fit <- fit_inter_fe
oof <- best_fit$pred %>%
transmute(pred = pred, obs = obs)
ggplot(oof, aes(x = obs, y = pred)) +
geom_point(alpha = 0.15) +
geom_abline(slope = 1, intercept = 0, linetype = 2) +
labs(
title = "Predicted vs. Actual (Out-of-Fold) — Interactions/FE Model",
x = "Actual log(Sale Price)", y = "Predicted log(Sale Price)"
) +
theme_minimal()
# Optional: nicely formatted comparison table for slides
cv_table %>%
mutate(
RMSE = round(RMSE, 3),
Rsq  = round(Rsq, 3),
MAE  = round(MAE, 3)
)
# Phase 6 — Model Diagnostics for the best linear spec (Interactions / FE)
# Refit the Interactions/FE model on the full dataset to get residuals cleanly
model_inter_fe <- lm(form_inter_fe, data = model_data_cv)
summary(model_inter_fe)  # optional: quick overview
# 6.1 Residual vs Fitted (linearity & homoscedasticity check)
par(mfrow = c(1, 1))
plot(model_inter_fe$fitted.values, resid(model_inter_fe),
pch = 16, col = rgb(0,0,0,0.25),
xlab = "Fitted values", ylab = "Residuals",
main = "Residuals vs Fitted")
# 6.1 Residual vs Fitted (linearity & homoscedasticity check)
# 6.1 Residual vs Fitted (linearity & homoscedasticity check)
par(mfrow = c(1, 1))
plot(
model_inter_fe$fitted.values,
resid(model_inter_fe),
pch = 16, col = rgb(0, 0, 0, 0.25),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals vs Fitted"
)
abline(h = 0, lty = 2, col = 2)
# 6.2 Q-Q plot (normality of residuals)
qqnorm(resid(model_inter_fe), main = "Q-Q Plot of Residuals")
qqline(resid(model_inter_fe), col = 2, lwd = 2)
# 6.3 Scale-Location (residual spread vs fitted — variance stabilization)
plot(model_inter_fe, which = 3)  # Scale-Location built-in plot
# 6.4 Cook's distance (influential observations)
n <- nrow(model_data_cv)
cook <- cooks.distance(model_inter_fe)
thr <- 4 / n  # common rule of thumb
# Basic summary
cat("Cook's D threshold (4/n):", round(thr, 6), "\n")
cat("Number of points above threshold:", sum(cook > thr), "\n")
# Plot Cook's D with threshold
plot(cook, type = "h", col = "gray40",
main = "Cook's Distance", ylab = "Cook's D")
abline(h = thr, col = 2, lty = 2)
# Optionally list top-10 influential indices
head(order(cook, decreasing = TRUE), 10)
# 6.5 (Optional) Heteroskedasticity test (Breusch–Pagan)
if (!requireNamespace("lmtest", quietly = TRUE)) install.packages("lmtest")
library(lmtest)
bptest(model_inter_fe)
# 6.6 (Optional) Multicollinearity check (VIF)
if (!requireNamespace("car", quietly = TRUE)) install.packages("car")
library(car)
vif_vals_diag <- car::vif(model_inter_fe)
sort(vif_vals_diag, decreasing = TRUE)[1:10]
