# Calculate k nearest neighbors and distances
nn_result <- get.knnx(abandoned_coords, fishnet_coords, k = 3)
# Add to fishnet
fishnet <- fishnet %>%
mutate(
abandoned_cars.nn = rowMeans(nn_result$nn.dist)
)
cat("✓ Calculated nearest neighbor distances\n")
summary(fishnet$abandoned_cars.nn)
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {
# Create spatial weights
coords <- st_coordinates(st_centroid(data))
neighbors <- knn2nb(knearneigh(coords, k = k))
weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
# Calculate Local Moran's I
local_moran <- localmoran(data[[variable]], weights)
# Classify clusters
mean_val <- mean(data[[variable]], na.rm = TRUE)
data %>%
mutate(
local_i = local_moran[, 1],
p_value = local_moran[, 5],
is_significant = p_value < 0.05,
moran_class = case_when(
!is_significant ~ "Not Significant",
local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
TRUE ~ "Not Significant"
)
)
}
# Apply to abandoned cars
fishnet <- calculate_local_morans(fishnet, "abandoned_cars", k = 5)
#| fig-width: 8
#| fig-height: 6
# Visualize hot spots
ggplot() +
geom_sf(
data = fishnet,
aes(fill = moran_class),
color = NA
) +
scale_fill_manual(
values = c(
"High-High" = "#d7191c",
"High-Low" = "#fdae61",
"Low-High" = "#abd9e9",
"Low-Low" = "#2c7bb6",
"Not Significant" = "gray90"
),
name = "Cluster Type"
) +
labs(
title = "Local Moran's I: Abandoned Car Clusters",
subtitle = "High-High = Hot spots of disorder"
) +
theme_crime()
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
filter(moran_class == "High-High") %>%
st_centroid()
# Calculate distance from each cell to nearest hot spot
if (nrow(hotspots) > 0) {
fishnet <- fishnet %>%
mutate(
dist_to_hotspot = as.numeric(
st_distance(st_centroid(fishnet), hotspots %>% st_union())
)
)
cat("✓ Calculated distance to abandoned car hot spots\n")
cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
fishnet <- fishnet %>%
mutate(dist_to_hotspot = 0)
cat("⚠ No significant hot spots found\n")
}
# Join district information to fishnet
fishnet <- st_join(
fishnet,
policeDistricts,
join = st_within,
left = TRUE
) %>%
filter(!is.na(District))  # Remove cells outside districts
cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
cat("  - Cells:", nrow(fishnet), "\n")
# Create clean modeling dataset
fishnet_model <- fishnet %>%
st_drop_geometry() %>%
dplyr::select(
uniqueID,
District,
countBurglaries,
abandoned_cars,
abandoned_cars.nn,
dist_to_hotspot
) %>%
na.omit()  # Remove any remaining NAs
cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
cat("  - Variables:", ncol(fishnet_model), "\n")
# Fit Poisson regression
model_poisson <- glm(
countBurglaries ~ abandoned_cars + abandoned_cars.nn +
dist_to_hotspot,
data = fishnet_model,
family = "poisson"
)
# Summary
summary(model_poisson)
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) /
model_poisson$df.residual
cat("Dispersion parameter:", round(dispersion, 2), "\n")
cat("Rule of thumb: >1.5 suggests overdispersion\n")
if (dispersion > 1.5) {
cat("⚠ Overdispersion detected! Consider Negative Binomial model.\n")
} else {
cat("✓ Dispersion looks okay for Poisson model.\n")
}
# Fit Negative Binomial model
model_nb <- glm.nb(
countBurglaries ~ abandoned_cars + abandoned_cars.nn +
dist_to_hotspot,
data = fishnet_model
)
# Summary
summary(model_nb)
# Compare AIC (lower is better)
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
# Get unique districts
districts <- unique(fishnet_model$District)
cv_results <- tibble()
cat("Running LOGO Cross-Validation...\n")
for (i in seq_along(districts)) {
test_district <- districts[i]
# Split data
train_data <- fishnet_model %>% filter(District != test_district)
test_data <- fishnet_model %>% filter(District == test_district)
# Fit model on training data
model_cv <- glm.nb(
countBurglaries ~ abandoned_cars + abandoned_cars.nn +
dist_to_hotspot,
data = train_data
)
# Predict on test data
test_data <- test_data %>%
mutate(
prediction = predict(model_cv, test_data, type = "response")
)
# Calculate metrics
mae <- mean(abs(test_data$countBurglaries - test_data$prediction))
rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
# Store results
cv_results <- bind_rows(
cv_results,
tibble(
fold = i,
test_district = test_district,
n_test = nrow(test_data),
mae = mae,
rmse = rmse
)
)
cat("  Fold", i, "/", length(districts), "- District", test_district,
"- MAE:", round(mae, 2), "\n")
}
# Overall results
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
# Show results
cv_results %>%
arrange(desc(mae)) %>%
kable(
digits = 2,
caption = "LOGO CV Results by District"
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Fit final model on all data
final_model <- glm.nb(
countBurglaries ~ abandoned_cars + abandoned_cars.nn +
dist_to_hotspot,
data = fishnet_model
)
# Add predictions back to fishnet
fishnet <- fishnet %>%
mutate(
prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
)
# Also add KDE predictions (normalize to same scale as counts)
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries, na.rm = TRUE)
fishnet <- fishnet %>%
mutate(
prediction_kde = (kde_value / kde_sum) * count_sum
)
#| fig-width: 12
#| fig-height: 4
# Create three maps
p1 <- ggplot() +
geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
labs(title = "Actual Burglaries") +
theme_crime()
p2 <- ggplot() +
geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
labs(title = "Model Predictions (Neg. Binomial)") +
theme_crime()
p3 <- ggplot() +
geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
labs(title = "KDE Baseline Predictions") +
theme_crime()
p1 + p2 + p3 +
plot_annotation(
title = "Actual vs. Predicted Burglaries",
subtitle = "Does our complex model outperform simple KDE?"
)
# Calculate performance metrics
comparison <- fishnet %>%
st_drop_geometry() %>%
filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
summarize(
model_mae = mean(abs(countBurglaries - prediction_nb)),
model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
kde_mae = mean(abs(countBurglaries - prediction_kde)),
kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))
)
comparison %>%
pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
separate(metric, into = c("approach", "metric"), sep = "_") %>%
pivot_wider(names_from = metric, values_from = value) %>%
kable(
digits = 2,
caption = "Model Performance Comparison"
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-width: 10
#| fig-height: 5
# Calculate errors
fishnet <- fishnet %>%
mutate(
error_nb = countBurglaries - prediction_nb,
error_kde = countBurglaries - prediction_kde,
abs_error_nb = abs(error_nb),
abs_error_kde = abs(error_kde)
)
# Map errors
p1 <- ggplot() +
geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
scale_fill_gradient2(
name = "Error",
low = "#2166ac", mid = "white", high = "#b2182b",
midpoint = 0,
limits = c(-10, 10)
) +
labs(title = "Model Errors (Actual - Predicted)") +
theme_crime()
p2 <- ggplot() +
geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
scale_fill_viridis_c(name = "Abs. Error", option = "magma") +
labs(title = "Absolute Model Errors") +
theme_crime()
p1 + p2
# Create nice summary table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
mutate(
across(where(is.numeric), ~round(., 3))
)
model_summary %>%
kable(
caption = "Final Negative Binomial Model Coefficients (Exponentiated)",
col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
footnote(
general = "Rate ratios > 1 indicate positive association with burglary counts."
)
library(tidyverse)
library(sf)
library(here)
library(viridis)
library(spatstat.geom)
library(spatstat.explore)
library(terra)
library(spdep)
library(FNN)
library(MASS)
library(patchwork)
library(knitr)
library(kableExtra)
# Load Chicago boundary and police districts
chicagoBoundary <- st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") |> st_transform('ESRI:102271')
policeDistricts <- st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") |> st_transform('ESRI:102271') |> select(District = dist_num)
library(tidyverse)
library(sf)
library(here)
library(viridis)
library(spatstat.geom)
library(spatstat.explore)
library(terra)
library(spdep)
library(FNN)
library(MASS)
library(patchwork)
library(knitr)
library(kableExtra)
# Load Chicago boundary and police districts
chicagoBoundary <- st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") |> st_transform('ESRI:102271')
# Load police districts (for spatial CV)
policeDistricts <-
st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
st_transform('ESRI:102271') %>%
rename_with(~ "District", .cols = matches("dist|DIST|DIST_NUM|dist_num"))
library(tidyverse)
library(sf)
library(here)
library(viridis)
library(spatstat.geom)
library(spatstat.explore)
library(terra)
library(spdep)
library(FNN)
library(MASS)
library(patchwork)
library(knitr)
library(kableExtra)
# Load Chicago boundary and police districts
chicagoBoundary <- st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") |> st_transform('ESRI:102271')
# Load police districts (for spatial CV)
# Load Chicago police districts safely
policeDistricts <- {
tmp <- st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
st_transform('ESRI:102271')
dist_col <- names(tmp)[grepl("dist", names(tmp), ignore.case = TRUE)][1]
tmp %>%
rename(District = all_of(dist_col)) %>%
select(District, geometry)
}
library(tidyverse)
library(sf)
library(here)
library(viridis)
library(spatstat.geom)
library(spatstat.explore)
library(terra)
library(spdep)
library(FNN)
library(MASS)
library(patchwork)
library(knitr)
library(kableExtra)
# Load Chicago boundary and police districts
chicagoBoundary <- st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") |> st_transform('ESRI:102271')
# Load police districts (for spatial CV)
# Load Chicago police districts safely
tmp <- st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
st_transform('ESRI:102271')
# 自动找含 "dist" 的列名（DISTRICT / DIST_NUM / dist_num 等）
dist_col <- names(tmp)[grepl("dist", names(tmp), ignore.case = TRUE)][1]
if (is.na(dist_col)) stop("No district-like column found.")
library(rlang)  # 用于 sym()
policeDistricts <- tmp %>%
dplyr::rename(District = !!sym(dist_col)) %>%   # 一定用 dplyr::
dplyr::select(District, geometry)               # 一定用 dplyr::
cat("✓ Loaded police districts successfully\n")
print(head(policeDistricts))
cat("✓ Loaded police districts\n")
print(names(policeDistricts))
# Load 311 Graffiti Removal (2017)
graffiti <- read_csv(here("data","311_graffiti_2017.csv"), show_col_types = FALSE) |>
filter(!is.na(Latitude), !is.na(Longitude)) |>
st_as_sf(coords = c("Longitude","Latitude"), crs = 4326) |>
st_transform('ESRI:102271')
# Load required packages
library(tidyverse)   # For data manipulation and visualization
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv(here("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv"))
library(here)        # For project-relative file paths
# Set random seed for reproducibility
set.seed(2025)
# Configure visualization theme
theme_set(theme_minimal())
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv(here("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv"))
# Examine data structure
glimpse(recidivism_data)
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv(here("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv"))
# Examine data structure
glimpse(recidivism_data)
# Check outcome variable distribution
table(recidivism_data$Recidivism_Within_3years, useNA = "ifany")
# Calculate overall recidivism rate
mean(recidivism_data$Recidivism_Within_3years, na.rm = TRUE)
# Examine recidivism rates by race
recidivism_data %>%
filter(!is.na(Race), !is.na(Recidivism_Within_3years)) %>%
group_by(Race) %>%
summarise(
n = n(),
recidivism_rate = mean(Recidivism_Within_3years),
avg_age = mean(Age_at_Release, na.rm = TRUE),
avg_prior_felonies = mean(Prior_Arrest_Episodes_Felony, na.rm = TRUE)
) %>%
arrange(desc(recidivism_rate))
# Clean and prepare data for modeling
model_data <- recidivism_data %>%
# Create binary outcome variable (must be numeric for glm)
mutate(recidivism = as.integer(Recidivism_Within_3years == TRUE)) %>%
# Select relevant features
select(
recidivism,                         # Outcome
Age_at_Release,                     # Demographics
Gender,
Race,
Gang_Affiliated,                    # Risk factors
Dependents,
Prior_Arrest_Episodes_Felony,       # Criminal history
Prior_Arrest_Episodes_Violent,
Prior_Conviction_Episodes_Prop,
Condition_MH_SA,                    # Mental health / substance abuse
Supervision_Risk_Score_First,       # Official risk score
Percent_Days_Employed,              # Economic factors
Education_Level                     # Education
) %>%
# Remove missing values (in practice, consider imputation)
na.omit()
# Check final sample characteristics
cat("Final sample size:", nrow(model_data), "individuals\n")
cat("Recidivism rate:", round(mean(model_data$recidivism), 3), "\n\n")
# Sample size by race
model_data %>%
count(Race) %>%
arrange(desc(n))
# Create stratified train-test split (maintains outcome distribution)
trainIndex <- createDataPartition(
y = model_data$recidivism,
p = 0.70,           # 70% training, 30% testing
list = FALSE
)
train_data <- model_data[trainIndex, ]
test_data <- model_data[-trainIndex, ]
# Verify split preserves outcome distribution
cat("Training set:\n")
cat("  N =", nrow(train_data), "\n")
cat("  Recidivism rate =", round(mean(train_data$recidivism), 3), "\n\n")
cat("Test set:\n")
cat("  N =", nrow(test_data), "\n")
cat("  Recidivism rate =", round(mean(test_data$recidivism), 3), "\n\n")
# Fit logistic regression model
# Note: We're excluding Race from predictors to avoid direct discrimination,
# but this doesn't eliminate bias (proxy variables may exist)
logit_model <- glm(
recidivism ~ Age_at_Release +
Dependents +
Gang_Affiliated +
Prior_Arrest_Episodes_Felony +
Prior_Arrest_Episodes_Violent +
Prior_Conviction_Episodes_Prop +
Percent_Days_Employed +
Supervision_Risk_Score_First,
data = train_data,
family = "binomial"  # Specifies logistic regression
)
# View model summary
summary(logit_model)
# Interpret coefficients as odds ratios
exp(coef(logit_model))
# Generate predicted probabilities on test set
test_data <- test_data %>%
mutate(
predicted_prob = predict(logit_model, newdata = test_data, type = "response")
)
# Examine distribution of predicted probabilities
summary(test_data$predicted_prob)
# Load required packages
library(tidyverse)   # For data manipulation and visualization
library(caret)       # For model training and confusion matrices
library(pROC)        # For ROC curves and AUC
library(here)        # For project-relative file paths
# Set random seed for reproducibility
set.seed(2025)
# Configure visualization theme
theme_set(theme_minimal())
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv(here("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv"))
# Examine data structure
glimpse(recidivism_data)
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv")
# Examine data structure
glimpse(recidivism_data)
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv")
# Load Georgia Department of Corrections recidivism data
# Source: National Institute of Justice Recidivism Challenge
recidivism_data <- read_csv("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv")
