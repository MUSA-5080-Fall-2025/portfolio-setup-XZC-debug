---
title: "Week 5 Notes"
date: "2025-10-06"
---

## Key Concepts Learned
- [List main concepts from lecture]
- [Technical skills covered]


quiz 3 questions
1.src_transform 
src_set_crs
2..refer to the previous variable
3.st_filter(buffer,.predicate=st_intersects)    select complete features
st_intersection(buffer)   clips geometries to create new shapes


Introduction to Linear Regression
Part 1: The Statistical Learning Framework
Formalizing the Relationship
f represents the true relationship between predictors and outcome
Parametric (blue): We assume f is linear, then estimate β₀ and β₁
Non-parametric (green): We let the data determine the shape of f
t-statistic: How many standard errors away from 0?
p-value: Probability of seeing our estimate if H₀ is true

Part 2: Two Different Goals

Part 3: Building Your First Model
no hypothesis

Part 4: Model Evaluation
R²
Underfitting: Model too simple (high bias)
Good fit: Captures pattern without noise
Overfitting: Memorizes training data (high variance)
Cross-Validation

Part 5: Checking Assumptions
Assumption 1: Linearity
What we assume: Relationship is actually linear
How to check: Residual plot

Assumption 2: Constant Variance
Heteroscedasticity: Variance changes across X
Impact: Standard errors are wrong → p-values misleading
Assumption: Normality of Residuals
Q-Q Plot

Assumption 3: No Multicollinearity
For multiple regression: Predictors shouldn’t be too correlated
Why it matters: Coefficients become unstable, hard to interpret

Assumption 4: No Influential Outliers
Not all outliers are problems - only those with high leverage AND large residuals

Part 6: Improving the Model
## Coding Techniques
- [New R functions or approaches]
- [Quarto features learned]

original codes:
# county
county_stats <- tracts_in_cnty_num %>%
  sf::st_drop_geometry() %>%
  dplyr::group_by(county_name) %>%
  dplyr::summarise(
    n_tracts          = dplyr::n(),
    pop_total         = sum(pop_total, na.rm = TRUE),
    median_income_wt  = safe_wmean(median_income, pop_total),
    pov_rate_wt       = safe_wmean(pov_rate, pop_total),
    share65_wt        = safe_wmean(share65, pop_total),
    nov_percapita_wt  = safe_wmean(nov_percapita, pop_total),
    vuln_index_wt     = safe_wmean(vuln_index, pop_total),
    min_dist_hosp_km  = suppressWarnings(min(dist_hosp_km, na.rm = TRUE))
  ) %>%
  dplyr::ungroup()
  


## Questions & Challenges
- [What I didn't fully understand]
- [Areas needing more practice]

The issue occurred because, within the county-level aggregation step, the population-weighted averages were calculated after redefining pop_total inside summarise(). This caused the weight variable to be replaced by the newly aggregated (scalar) population value instead of the original tract-level vector, leading to all vuln_index_wt results becoming NA. After debugging, it was confirmed that tract-level vuln_index values were valid and that each county had sufficient valid data pairs. The fix was to reorder the summarization so that all weighted means were computed before redefining pop_total, and to explicitly reference columns using .data$ to avoid name masking. This correction restored proper population-weighted calculations and produced valid county-level vulnerability scores.

## Connections to Policy
- [How this week's content applies to real policy work]

## Reflection
- [What was most interesting]
- [How I'll apply this knowledge]
